#!/usr/bin/env python3
"""
å…¨é¢çš„æ„å›¾è¯†åˆ«æµ‹è¯•è„šæœ¬
æµ‹è¯•åŸºäºBERTçš„æ„å›¾è¯†åˆ«åœ¨å„ç§åœºæ™¯ä¸‹çš„æ€§èƒ½
"""

import sys
import os
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import json
import random
from src.legacy.dialog import DialogManager
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from config import Config

# åˆ›å»ºé…ç½®å®ä¾‹
config = Config()

class IntentTestSuite:
    """
    æ„å›¾è¯†åˆ«æµ‹è¯•å¥—ä»¶
    """
    
    def __init__(self):
        self.dialog_manager = DialogManager(use_bert_intent=True)
        self.test_results = {}
        
    def prepare_memory(self):
        """
        å‡†å¤‡å¯¹è¯è®°å¿†
        """
        return {
            'avaiable_nodes': list(self.dialog_manager.node_id2node_info.keys()),
            'user_input': "",
            'entities': {},
            'slot_filled': {},
            'state': {},
            'hit_intent': None,
            'hit_intent_score': 0
        }
    
    def load_test_data(self, file_path=None):
        """
        åŠ è½½æµ‹è¯•æ•°æ®
        """
        if file_path is None:
            file_path = os.path.join(config.ANNOTATION_DIR, "intent_annotation_merged.json")
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def get_all_intent_labels(self):
        """
        è·å–æ‰€æœ‰æ„å›¾æ ‡ç­¾
        """
        return list(self.dialog_manager.node_id2node_info.keys())
    
    def test_basic_intent_recognition(self, test_cases=None):
        """
        åŸºç¡€æ„å›¾è¯†åˆ«æµ‹è¯•
        """
        print("1. åŸºç¡€æ„å›¾è¯†åˆ«æµ‹è¯•")
        print("=" * 60)
        
        if not test_cases:
            test_cases = [
                "æˆ‘æœ‰æ³•å¾‹é—®é¢˜",
                "æˆ‘æƒ³å’¨è¯¢åŠ³åŠ¨åˆåŒé—®é¢˜",
                "å…¬å¸æ‹–æ¬ å·¥èµ„æ€ä¹ˆåŠ",
                "æˆ‘è¢«å…¬å¸è¾é€€äº†",
                "ç¦»å©šè´¢äº§æ€ä¹ˆåˆ†å‰²",
                "äº¤é€šäº‹æ•…èµ”å¿æ ‡å‡†",
                "æˆ¿äº§ç»§æ‰¿çº çº·",
                "ä¸“åˆ©ç”³è¯·æµç¨‹",
                "åˆåŒçº çº·æ€ä¹ˆè§£å†³",
                "è¡Œæ”¿å¤è®®ç¨‹åº",
                "è‘—ä½œæƒä¿æŠ¤æœŸé™",
                "é—äº§ç»§æ‰¿é¡ºåº",
                "åŒ»ç–—äº‹æ•…èµ”å¿",
                "æ°‘é—´å€Ÿè´·åˆ©ç‡",
                "åˆ‘äº‹æ¡ˆä»¶è¾©æŠ¤"
            ]
        
        memory = self.prepare_memory()
        results = []
        total_time = 0
        
        for test_input in test_cases:
            memory['user_input'] = test_input
            
            start_time = time.time()
            result = self.dialog_manager.intent_recognizer.intent_recognize(memory.copy())
            end_time = time.time()
            
            response_time = end_time - start_time
            total_time += response_time
            
            results.append({
                'input': test_input,
                'intent': result['hit_intent'],
                'score': result['hit_intent_score'],
                'response_time': response_time
            })
        
        avg_time = total_time / len(test_cases)
        
        # è¾“å‡ºç»“æœ
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.4f}ç§’")
        print("\nè¯¦ç»†ç»“æœ:")
        print(f"{'è¾“å…¥':<25} {'è¯†åˆ«æ„å›¾':<25} {'ç½®ä¿¡åº¦':<10} {'å“åº”æ—¶é—´(ms)':<15}")
        print("-" * 75)
        
        for res in results:
            print(f"{res['input']:<25} {res['intent']:<25} {res['score']:<10.4f} {res['response_time']*1000:<15.2f}")
        
        self.test_results['basic_test'] = {
            'avg_response_time': avg_time,
            'results': results
        }
        
        print()
    
    def test_classification_accuracy(self, test_size=0.2):
        """
        æµ‹è¯•åˆ†ç±»å‡†ç¡®ç‡
        """
        print("2. åˆ†ç±»å‡†ç¡®ç‡æµ‹è¯•")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        all_data = self.load_test_data()
        
        # éšæœºæ‰“ä¹±æ•°æ®
        random.shuffle(all_data)
        
        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        test_split = int(len(all_data) * test_size)
        test_data = all_data[:test_split]
        
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
        
        y_true = []
        y_pred = []
        scores = []
        memory = self.prepare_memory()
        
        for item in test_data:
            user_input = item['user_input']
            true_label = item['intent_label']
            
            memory['user_input'] = user_input
            result = self.dialog_manager.intent_recognizer.intent_recognize(memory.copy())
            
            pred_label = result['hit_intent']
            confidence = result['hit_intent_score']
            
            y_true.append(true_label)
            y_pred.append(pred_label)
            scores.append(confidence)
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(y_true, y_pred)
        
        # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average='weighted', zero_division=0
        )
        
        # è®¡ç®—æ²¡æœ‰è¢«è¯†åˆ«çš„æ ·æœ¬æ•°ï¼ˆè¿”å›Noneçš„æƒ…å†µï¼‰
        unrecognized = sum(1 for pred in y_pred if pred is None)
        
        # è¾“å‡ºç»“æœ
        print(f"åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"å¬å›ç‡: {recall:.4f}")
        print(f"F1å€¼: {f1:.4f}")
        print(f"æœªè¯†åˆ«æ ·æœ¬æ•°: {unrecognized} ({unrecognized/len(test_data):.2%})")
        
        # è¾“å‡ºè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred, zero_division=0))
        
        self.test_results['accuracy_test'] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'unrecognized': unrecognized,
            'y_true': y_true,
            'y_pred': y_pred
        }
        
        print()
    
    def test_confidence_thresholds(self, thresholds=[0.1, 0.2, 0.3, 0.4, 0.5], test_size=0.2):
        """
        æµ‹è¯•ä¸åŒç½®ä¿¡åº¦é˜ˆå€¼ä¸‹çš„æ€§èƒ½
        ä¼˜åŒ–ï¼šå…ˆè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¾—åˆ†ï¼Œå†å¯¹ä¸åŒé˜ˆå€¼è¿›è¡Œè¿‡æ»¤ï¼Œå‡å°‘é‡å¤è®¡ç®—
        """
        print("3. ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        all_data = self.load_test_data()
        random.shuffle(all_data)
        test_split = int(len(all_data) * test_size)
        test_data = all_data[:test_split]
        
        original_threshold = self.dialog_manager.intent_recognizer.confidence_threshold
        
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
        print(f"åŸå§‹ç½®ä¿¡åº¦é˜ˆå€¼: {original_threshold}")
        print()
        
        print(f"{'é˜ˆå€¼':<8} {'å‡†ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'æœªè¯†åˆ«ç‡':<10} {'å¹³å‡ç½®ä¿¡åº¦':<15}")
        print("-" * 53)
        
        # å…ˆè®¡ç®—æ‰€æœ‰æµ‹è¯•æ ·æœ¬çš„åŸå§‹ç»“æœï¼ˆä¸åº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼‰
        all_results = []
        memory = self.prepare_memory()
        
        # ä¸´æ—¶å°†é˜ˆå€¼è®¾ä¸º0ï¼Œè·å–æ‰€æœ‰é¢„æµ‹ç»“æœ
        self.dialog_manager.intent_recognizer.confidence_threshold = 0.0
        
        for item in test_data:
            user_input = item['user_input']
            true_label = item['intent_label']
            
            memory['user_input'] = user_input
            result = self.dialog_manager.intent_recognizer.intent_recognize(memory.copy())
            
            all_results.append({
                'true_label': true_label,
                'pred_label': result['hit_intent'],
                'confidence': result['hit_intent_score']
            })
        
        # æ¢å¤åŸå§‹é˜ˆå€¼
        self.dialog_manager.intent_recognizer.confidence_threshold = original_threshold
        
        threshold_results = []
        
        # å¯¹æ¯ä¸ªé˜ˆå€¼ï¼ŒåŸºäºé¢„è®¡ç®—çš„ç»“æœè¿›è¡Œåˆ†æ
        for threshold in thresholds:
            y_true = []
            y_pred = []
            total_confidence = 0
            recognized_count = 0
            
            for result in all_results:
                true_label = result['true_label']
                pred_label = result['pred_label']
                confidence = result['confidence']
                
                # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                if confidence >= threshold:
                    # ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼ï¼Œä¿ç•™é¢„æµ‹ç»“æœ
                    y_true.append(true_label)
                    y_pred.append(pred_label)
                    total_confidence += confidence
                    recognized_count += 1
                else:
                    # ç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼Œé¢„æµ‹ç»“æœä¸ºNone
                    y_true.append(true_label)
                    y_pred.append(None)
            
            # è®¡ç®—æŒ‡æ ‡
            # è¿‡æ»¤æ‰é¢„æµ‹ä¸ºNoneçš„æ ·æœ¬ï¼Œåªè®¡ç®—è¢«è¯†åˆ«æ ·æœ¬çš„å‡†ç¡®ç‡
            recognized_y_true = [true for true, pred in zip(y_true, y_pred) if pred is not None]
            recognized_y_pred = [pred for pred in y_pred if pred is not None]
            
            accuracy = accuracy_score(recognized_y_true, recognized_y_pred) if recognized_count > 0 else 0
            recall = accuracy if recognized_count > 0 else 0
            
            unrecognized_rate = (len(y_pred) - recognized_count) / len(y_pred)
            avg_confidence = total_confidence / recognized_count if recognized_count > 0 else 0
            
            threshold_results.append({
                'threshold': threshold,
                'accuracy': accuracy,
                'recall': recall,
                'unrecognized_rate': unrecognized_rate,
                'avg_confidence': avg_confidence
            })
            
            print(f"{threshold:<8} {accuracy:<10.4f} {recall:<10.4f} {unrecognized_rate:<10.4f} {avg_confidence:<15.4f}")
        
        self.test_results['threshold_test'] = threshold_results
        
        print()
    
    def test_robustness(self):
        """
        æµ‹è¯•æ¨¡å‹é²æ£’æ€§
        """
        print("4. é²æ£’æ€§æµ‹è¯•")
        print("=" * 60)
        
        # é²æ£’æ€§æµ‹è¯•ç”¨ä¾‹ï¼šåŒ…å«æ‹¼å†™é”™è¯¯ã€å£è¯­åŒ–è¡¨è¾¾ã€å™ªéŸ³ç­‰
        robustness_test_cases = [
            # æ­£å¸¸è¾“å…¥
            ("å…¬å¸æ‹–æ¬ å·¥èµ„æ€ä¹ˆåŠ", "æ­£å¸¸è¾“å…¥"),
            # æ‹¼å†™é”™è¯¯
            ("å…¬å¸æ‹–æ¬ å·¥å§¿æ€ä¹ˆåŠ", "æ‹¼å†™é”™è¯¯"),
            # å£è¯­åŒ–è¡¨è¾¾
            ("è€æ¿ä¸ç»™å·¥èµ„å’‹æ•´", "å£è¯­åŒ–è¡¨è¾¾"),
            # é•¿å¥
            ("æˆ‘åœ¨ä¸€å®¶å…¬å¸å·¥ä½œäº†ä¸‰å¹´ï¼Œç°åœ¨å…¬å¸æ‹–æ¬ æˆ‘ä¸‰ä¸ªæœˆçš„å·¥èµ„ï¼Œæˆ‘è¯¥æ€ä¹ˆåŠæ‰èƒ½è¦å›æˆ‘çš„å·¥èµ„", "é•¿å¥"),
            # çŸ­å¥
            ("å·¥èµ„æ‹–æ¬ ", "çŸ­å¥"),
            # å¸¦æ ‡ç‚¹ç¬¦å·
            ("å…¬å¸æ‹–æ¬ å·¥èµ„æ€ä¹ˆåŠï¼Ÿ", "å¸¦æ ‡ç‚¹ç¬¦å·"),
            # å¸¦è¡¨æƒ…ç¬¦å·
            ("å…¬å¸æ‹–æ¬ å·¥èµ„æ€ä¹ˆåŠğŸ˜¡", "å¸¦è¡¨æƒ…ç¬¦å·"),
            # æ–¹è¨€
            ("å…¬å¸æ‹–èµ·å·¥èµ„ä¸ç»™å’‹ä¸ªåŠ", "æ–¹è¨€"),
            # é‡å¤è¯è¯­
            ("å…¬å¸æ‹–æ¬ æ‹–æ¬ å·¥èµ„å·¥èµ„æ€ä¹ˆåŠ", "é‡å¤è¯è¯­"),
            # æ¨¡ç³Šè¡¨è¾¾
            ("æˆ‘æœ‰ä¸ªå…³äºå·¥ä½œçš„æ³•å¾‹é—®é¢˜", "æ¨¡ç³Šè¡¨è¾¾")
        ]
        
        memory = self.prepare_memory()
        results = []
        
        print(f"{'æµ‹è¯•ç±»å‹':<15} {'è¾“å…¥':<30} {'è¯†åˆ«æ„å›¾':<25} {'ç½®ä¿¡åº¦':<10}")
        print("-" * 80)
        
        for test_input, test_type in robustness_test_cases:
            memory['user_input'] = test_input
            result = self.dialog_manager.intent_recognizer.intent_recognize(memory.copy())
            
            results.append({
                'test_type': test_type,
                'input': test_input,
                'intent': result['hit_intent'],
                'score': result['hit_intent_score']
            })
            
            print(f"{test_type:<15} {test_input:<30} {result['hit_intent']:<25} {result['hit_intent_score']:<10.4f}")
        
        # è®¡ç®—é²æ£’æ€§å¾—åˆ†ï¼šèƒ½æ­£ç¡®è¯†åˆ«çš„æµ‹è¯•ç”¨ä¾‹æ¯”ä¾‹
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œè®¤ä¸ºåªè¦è¿”å›äº†æ„å›¾å°±æˆåŠŸ
        successful = sum(1 for res in results if res['intent'] is not None)
        robustness_score = successful / len(results)
        
        print(f"\né²æ£’æ€§å¾—åˆ†: {robustness_score:.4f} ({successful}/{len(results)})")
        
        self.test_results['robustness_test'] = {
            'score': robustness_score,
            'results': results
        }
        
        print()
    
    def test_category_performance(self, test_size=0.2):
        """
        æµ‹è¯•ä¸åŒç±»åˆ«ä¸Šçš„è¡¨ç°
        """
        print("5. ç±»åˆ«æ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        all_data = self.load_test_data()
        random.shuffle(all_data)
        test_split = int(len(all_data) * test_size)
        test_data = all_data[:test_split]
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        data_by_category = {}
        for item in test_data:
            category = item['intent_label']
            if category not in data_by_category:
                data_by_category[category] = []
            data_by_category[category].append(item)
        
        memory = self.prepare_memory()
        category_results = {}
        
        print(f"{'ç±»åˆ«':<30} {'æ ·æœ¬æ•°':<8} {'å‡†ç¡®ç‡':<10} {'å¹³å‡ç½®ä¿¡åº¦':<15} {'æœªè¯†åˆ«ç‡':<10}")
        print("-" * 73)
        
        for category, items in data_by_category.items():
            y_true = []
            y_pred = []
            total_confidence = 0
            recognized_count = 0
            
            for item in items:
                user_input = item['user_input']
                true_label = item['intent_label']
                
                memory['user_input'] = user_input
                result = self.dialog_manager.intent_recognizer.intent_recognize(memory.copy())
                
                pred_label = result['hit_intent']
                confidence = result['hit_intent_score']
                
                y_true.append(true_label)
                y_pred.append(pred_label)
                
                if pred_label is not None:
                    total_confidence += confidence
                    recognized_count += 1
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred) if len(y_true) > 0 else 0
            avg_confidence = total_confidence / recognized_count if recognized_count > 0 else 0
            unrecognized_rate = (len(y_pred) - recognized_count) / len(y_pred)
            
            category_results[category] = {
                'sample_count': len(items),
                'accuracy': accuracy,
                'avg_confidence': avg_confidence,
                'unrecognized_rate': unrecognized_rate
            }
            
            print(f"{category:<30} {len(items):<8} {accuracy:<10.4f} {avg_confidence:<15.4f} {unrecognized_rate:<10.4f}")
        
        self.test_results['category_performance'] = category_results
        
        print()
    
    def test_tfidf_comparison(self):
        """
        ä¸TF-IDFæ–¹æ³•çš„å¯¹æ¯”æµ‹è¯•
        """
        print("6. ä¸TF-IDFæ–¹æ³•å¯¹æ¯”æµ‹è¯•")
        print("=" * 60)
        
        test_cases = [
            "æˆ‘æœ‰æ³•å¾‹é—®é¢˜",
            "æˆ‘æƒ³å’¨è¯¢åŠ³åŠ¨åˆåŒé—®é¢˜",
            "å…¬å¸æ‹–æ¬ å·¥èµ„æ€ä¹ˆåŠ",
            "æˆ‘è¢«å…¬å¸è¾é€€äº†",
            "ç¦»å©šè´¢äº§æ€ä¹ˆåˆ†å‰²",
            "äº¤é€šäº‹æ•…èµ”å¿æ ‡å‡†",
            "æˆ¿äº§ç»§æ‰¿çº çº·",
            "ä¸“åˆ©ç”³è¯·æµç¨‹",
            "åˆåŒçº çº·æ€ä¹ˆè§£å†³",
            "è¡Œæ”¿å¤è®®ç¨‹åº"
        ]
        
        # åˆå§‹åŒ–ä¸¤ä¸ªDialogManagerï¼Œä¸€ä¸ªä½¿ç”¨BERTï¼Œä¸€ä¸ªä¸ä½¿ç”¨
        bert_dialog = DialogManager(use_bert_intent=True)
        tfidf_dialog = DialogManager(use_bert_intent=False)
        
        memory = self.prepare_memory()
        
        print(f"{'è¾“å…¥':<20} {'TF-IDFæ„å›¾':<25} {'TF-IDFç½®ä¿¡åº¦':<15} {'BERTæ„å›¾':<25} {'BERTç½®ä¿¡åº¦':<15} {'æ„å›¾æ˜¯å¦ä¸€è‡´':<10}")
        print("-" * 105)
        
        results = []
        
        for test_input in test_cases:
            memory['user_input'] = test_input
            
            # TF-IDFç»“æœ
            tfidf_result = tfidf_dialog.intent_recognizer.intent_recognize(memory.copy())
            tfidf_intent = tfidf_result['hit_intent']
            tfidf_score = tfidf_result['hit_intent_score']
            
            # BERTç»“æœ
            bert_result = bert_dialog.intent_recognizer.intent_recognize(memory.copy())
            bert_intent = bert_result['hit_intent']
            bert_score = bert_result['hit_intent_score']
            
            # æ£€æŸ¥æ˜¯å¦ä¸€è‡´
            match = "âœ“" if tfidf_intent == bert_intent else "âœ—"
            
            results.append({
                'input': test_input,
                'tfidf_intent': tfidf_intent,
                'tfidf_score': tfidf_score,
                'bert_intent': bert_intent,
                'bert_score': bert_score,
                'match': match
            })
            
            print(f"{test_input:<20} {tfidf_intent:<25} {tfidf_score:<15.4f} {bert_intent:<25} {bert_score:<15.4f} {match:<10}")
        
        # è®¡ç®—ä¸€è‡´ç‡
        match_count = sum(1 for res in results if res['match'] == "âœ“")
        match_rate = match_count / len(results)
        
        print(f"\næ„å›¾ä¸€è‡´ç‡: {match_rate:.2%} ({match_count}/{len(results)})")
        
        self.test_results['tfidf_comparison'] = {
            'match_rate': match_rate,
            'results': results
        }
        
        print()
    
    def generate_test_report(self):
        """
        ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        """
        print("7. æµ‹è¯•æŠ¥å‘Šæ€»ç»“")
        print("=" * 60)
        
        print("æµ‹è¯•ç»“æœæ€»ç»“:")
        
        if 'accuracy_test' in self.test_results:
            acc_test = self.test_results['accuracy_test']
            print(f"- åˆ†ç±»å‡†ç¡®ç‡: {acc_test['accuracy']:.4f}")
            print(f"- ç²¾ç¡®ç‡: {acc_test['precision']:.4f}")
            print(f"- å¬å›ç‡: {acc_test['recall']:.4f}")
            print(f"- F1å€¼: {acc_test['f1']:.4f}")
            print(f"- æœªè¯†åˆ«æ ·æœ¬ç‡: {acc_test['unrecognized']/len(acc_test['y_true']):.2%}")
        
        if 'basic_test' in self.test_results:
            basic_test = self.test_results['basic_test']
            print(f"- å¹³å‡å“åº”æ—¶é—´: {basic_test['avg_response_time']:.4f}ç§’")
        
        if 'robustness_test' in self.test_results:
            robustness_test = self.test_results['robustness_test']
            print(f"- é²æ£’æ€§å¾—åˆ†: {robustness_test['score']:.4f}")
        
        if 'threshold_test' in self.test_results:
            threshold_test = self.test_results['threshold_test']
            best_threshold = max(threshold_test, key=lambda x: x['accuracy'])
            print(f"- æœ€ä½³ç½®ä¿¡åº¦é˜ˆå€¼: {best_threshold['threshold']} (å‡†ç¡®ç‡: {best_threshold['accuracy']:.4f})")
        
        if 'tfidf_comparison' in self.test_results:
            tfidf_comp = self.test_results['tfidf_comparison']
            print(f"- ä¸TF-IDFæ„å›¾ä¸€è‡´ç‡: {tfidf_comp['match_rate']:.2%}")
        
        print("\næµ‹è¯•å®Œæˆï¼")
        
        # ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶ï¼Œåªä¿å­˜å…³é”®ç»Ÿè®¡ç»“æœï¼Œä¸ä¿å­˜åŸå§‹é¢„æµ‹ç»“æœ
        # è¿™æ ·å¯ä»¥é¿å…JSONåºåˆ—åŒ–é—®é¢˜
        simple_results = {
            "summary": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "model": "BGE + XGBoost"
            }
        }
        
        # ä¿å­˜åˆ†ç±»å‡†ç¡®ç‡æµ‹è¯•ç»“æœ
        if 'accuracy_test' in self.test_results:
            acc_test = self.test_results['accuracy_test']
            simple_results["accuracy_test"] = {
                "accuracy": float(acc_test['accuracy']),
                "precision": float(acc_test['precision']),
                "recall": float(acc_test['recall']),
                "f1": float(acc_test['f1']),
                "unrecognized_count": acc_test['unrecognized'],
                "test_samples": len(acc_test['y_true'])
            }
        
        # ä¿å­˜åŸºç¡€æµ‹è¯•ç»“æœ
        if 'basic_test' in self.test_results:
            basic_test = self.test_results['basic_test']
            simple_results["basic_test"] = {
                "avg_response_time": float(basic_test['avg_response_time']),
                "test_cases": len(basic_test['results'])
            }
        
        # ä¿å­˜é²æ£’æ€§æµ‹è¯•ç»“æœ
        if 'robustness_test' in self.test_results:
            robustness_test = self.test_results['robustness_test']
            simple_results["robustness_test"] = {
                "score": float(robustness_test['score']),
                "test_cases": len(robustness_test['results']),
                "successful_cases": sum(1 for res in robustness_test['results'] if res['intent'] is not None)
            }
        
        # ä¿å­˜ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•ç»“æœ
        if 'threshold_test' in self.test_results:
            threshold_test = self.test_results['threshold_test']
            simple_results["threshold_test"] = {
                "thresholds": [float(t['threshold']) for t in threshold_test],
                "best_threshold": float(max(threshold_test, key=lambda x: x['accuracy'])['threshold']),
                "best_accuracy": float(max(threshold_test, key=lambda x: x['accuracy'])['accuracy'])
            }
        
        # ä¿å­˜ä¸TF-IDFå¯¹æ¯”ç»“æœ
        if 'tfidf_comparison' in self.test_results:
            tfidf_comp = self.test_results['tfidf_comparison']
            simple_results["tfidf_comparison"] = {
                "match_rate": float(tfidf_comp['match_rate']),
                "test_cases": len(tfidf_comp['results']),
                "matches": sum(1 for res in tfidf_comp['results'] if res['match'] == "âœ“")
            }
        
        # ä¿å­˜ç±»åˆ«æ€§èƒ½æµ‹è¯•ç»“æœ
        if 'category_performance' in self.test_results:
            category_perf = self.test_results['category_performance']
            simple_results["category_performance"] = {
                "categories": [{
                    "name": category,
                    "samples": data['sample_count'],
                    "accuracy": float(data['accuracy']),
                    "avg_confidence": float(data['avg_confidence']),
                    "unrecognized_rate": float(data['unrecognized_rate'])
                } for category, data in category_perf.items()]
            }
        
        with open("test_results.json", 'w', encoding='utf-8') as f:
            json.dump(simple_results, f, ensure_ascii=False, indent=2)
        
        print("æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° test_results.json")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("æ„å›¾è¯†åˆ«å…¨é¢æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    print("æµ‹è¯•åŸºäºBERTçš„æ³•å¾‹æ„å›¾è¯†åˆ«åœ¨å„ç§åœºæ™¯ä¸‹çš„æ€§èƒ½")
    print()
    
    # åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
    test_suite = IntentTestSuite()
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_suite.test_basic_intent_recognition()
    test_suite.test_classification_accuracy()
    test_suite.test_confidence_thresholds()
    test_suite.test_robustness()
    test_suite.test_category_performance()
    test_suite.test_tfidf_comparison()
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    test_suite.generate_test_report()

if __name__ == "__main__":
    main()
